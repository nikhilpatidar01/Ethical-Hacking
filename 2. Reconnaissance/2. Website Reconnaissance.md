
# ğŸŒ Website Reconnaissance

**Website reconnaissance** is the process of gathering publicly available information about a website, its infrastructure, and its online presence. It is a common initial step in **penetration testing**, **ethical hacking**, **SEO**, **research**, and **competitive analysis**.

---

## ğŸ“˜ What is Website Reconnaissance?

Website reconnaissance involves collecting detailed information about a target website or web application. The purpose is to understand its:

* Structure and architecture
* Underlying technologies
* Possible vulnerabilities or exposure points

---

## ğŸ¯ Why Perform Website Reconnaissance?

* **Security Assessment**: Identify vulnerabilities and attack surfaces.
* **Competitive Analysis**: Understand a competitorâ€™s tech stack and strategies.
* **SEO/Marketing Research**: Evaluate site structure, indexing, and backlinks.
* **Compliance Monitoring**: Detect leaked sensitive data and privacy issues.

---

## ğŸ§° Typical Activities

* **Gather Basic Details**: IP address, DNS records, WHOIS data, SSL certificates
* **Collect Metadata**: Titles, meta tags, CMS/frameworks, JS libraries
* **Identify Exposed Information**: Emails, usernames, hidden paths
* **Spidering and Crawling**: Link mapping and structure analysis
* **Historical Analysis**: View past versions using archive services

---

## ğŸ”§ Key Tools Used

| Category      | Tools/Examples                        |
| ------------- | ------------------------------------- |
| Online Recon  | Shodan, Censys, BuiltWith             |
| Crawlers      | Screaming Frog, XML Sitemap Generator |
| CLI Tools     | `wget`, `HTTrack`                     |
| Archive Tools | Archive.org, CachedView               |

---

# ğŸ—‚ï¸ 1. Visit the Target Website

Manually browse to extract:

* Names of employees
* Office addresses and phone numbers
* Email addresses and usernames (e.g., in blogs/comments)

---

# ğŸ“„ 2. Find Important Files

### ğŸ“œ `sitemap.html`

Human-readable site index showing navigation structure.

### ğŸ—‚ï¸ `sitemap.xml`

Machine-readable sitemap for search engine indexing.

### ğŸš« `robots.txt`

Indicates restricted areas for web crawlers.

---

### ğŸ”— Example Sitemap URLs

| Website             | Sitemap URL                                                |
| ------------------- | ---------------------------------------------------------- |
| India TV            | `https://www.indiatv.in/cms/sitemap.html`                  |
| MCA India           | `https://www.mca.gov.in/MinistryV2/sitemap.html`           |
| Telangana Transport | `https://www.transport.telangana.gov.in/html/sitemap.html` |

---

### ğŸŒ Online Sitemap Generator

Use online tools to generate/view sitemaps:

* [https://www.xml-sitemaps.com](https://www.xml-sitemaps.com)

---

# ğŸ•¸ï¸ 3. Spidering and Crawling â€” Create Sitemap

## ğŸ•·ï¸ What is Spidering?

**Spidering** is the process of using automated bots to systematically follow links across a website.

### ğŸ” Used For:

* Discovering internal & external links
* Detecting hidden directories/pages
* Mapping the structure of the site

### Key Features of Spidering

### Purpose

* Gather comprehensive information about a website, including:

  * URLs
  * Query parameters
  * Resources such as images, scripts, or files

### Tools

* Commonly used spidering tools include:

  * **Burp Suite**
  * **OWASP ZAP**
  * **Arachni**

### Use Case

* Discover pages or endpoints that are not directly linked from the main site, such as:

  * Admin panels
  * Hidden forms
  * Backup or old files

---

### Spidering Process

1. **Starting Point**

   * Begin with a *seed URL* (usually the homepage or a known entry point).

2. **Traversal**

   * Follow all hyperlinks found on each page, including:

     * Internal links (same domain)
     * External links (if in scope)

3. **Data Logging**

   * Record all discovered:

     * URLs
     * Query parameters
     * Dynamic endpoints (e.g., URLs with parameters or REST API endpoints)

4. **Scope Control**

   * Define the scope to avoid crawling irrelevant or unwanted pages, for example:

     * Exclude third-party domains
     * Avoid non-targeted subdomains

---

### Common Challenges

* **Dynamic Content**

  * Websites heavily reliant on JavaScript may need specialized tools or headless browsers (like Puppeteer or Selenium) to render and crawl content.

* **Access Restrictions**

  * Some areas may be protected by:

    * Authentication (login required)
    * IP restrictions or firewall rules


---

## ğŸŒ€ What is Crawling?

**Crawling** is a more comprehensive process. It includes spidering but also analyzes content.

### ğŸ” Used For:

* Downloading page data
* Extracting metadata (titles, headers, scripts)
* Identifying vulnerabilities or hidden assets

> âš ï¸ **Note**: Spidering = link mapping; Crawling = link + content analysis


# Crawling

### Definition

Crawling is a broader, systematic process of browsing and indexing content either across the entire internet or within a specific website. In penetration testing, crawling helps identify publicly accessible resources to provide a comprehensive view of the attack surface.

---

### Key Features of Crawling

* **Purpose:**
  Build a detailed index of a websiteâ€™s content for further analysis and testing.

* **Tools:**
  Common tools used for crawling include:

  * **Burp Suiteâ€™s crawler**
  * **OWASP ZAP**
  * Custom scripts (e.g., Python scripts using libraries like `requests` and `BeautifulSoup`)

* **Use Case:**

  * Widely used in SEO and digital marketing for content indexing.
  * In security testing, it helps discover overlooked files, directories, and resources that might be vulnerable.

---

### Crawling Process

1. **Discovery:**
   Systematically fetch pages, parse their content, and extract all links.

2. **Depth and Breadth Configuration:**

   * **Depth:** How many levels of links to follow from the starting page (e.g., link from homepage â†’ second-level pages â†’ deeper levels).
   * **Breadth:** How many different paths or branches across the site to explore.

3. **Data Collection:**
   Identify and record accessible resources, including:

   * Static content (HTML pages, images, scripts)
   * Dynamic content (pages generated via server-side scripts or APIs)


---

### ğŸŒ Online Tools for Sitemap & Crawling

* [XML Sitemaps Generator](https://www.xml-sitemaps.com)
* [ConvertCSV URL Extractor](https://www.convertcsv.com/url-extractor.htm)
* [Screaming Frog SEO Spider](https://www.screamingfrog.co.uk/seo-spider/)

---

# ğŸ› ï¸ 4. Web Crawling Tools

## âš”ï¸ Using **Katana**

Katana is a fast, next-gen web crawler used for recon and link discovery.

### ğŸ”§ Installation

```bash
git clone https://github.com/projectdiscovery/katana.git
cd katana/cmd/katana
go build
```

### â–¶ï¸ Usage

```bash
katana -u https://www.armourinfosec.com
```

---

## ğŸ§­ Using **Dirhunt**

**Dirhunt** scans websites for directories and hidden files.

### ğŸ”§ Installation

```bash
pip install dirhunt
```

### â„¹ï¸ Help Menu

```bash
dirhunt --help
```

### â–¶ï¸ Basic Scan

```bash
dirhunt https://www.armourinfosec.com
```

### ğŸ’¾ Save Output to File

```bash
dirhunt https://www.armourinfosec.com --to-file /tmp/output.json
```

---

# ğŸ’¾ 5. Locally Mirror the Website â€” Read Source Code

## ğŸ§² Using `wget` (Linux)

```bash
wget -m https://www.armourinfosec.com
```

> `-m` stands for **mirror**, enabling recursive download of pages and resources.

---

## ğŸ–¥ï¸ Using **HTTrack**

### âœ… On Linux (Kali, Debian, Ubuntu)

**Install:**

```bash
apt install httrack
```

**Run:**

```bash
httrack
```

Follow the interactive CLI to start mirroring.

---

### âœ… On Windows

ğŸ”— [Download HTTrack for Windows](https://www.httrack.com/page/2/en/index.html)

**Steps:**

1. Launch **WinHTTrack Website Copier**
2. Name your project
3. Enter the website URL
4. Choose a local folder
5. Click **Start**

---

# ğŸ•°ï¸ 6. Wayback Machine â€” View Old Versions of Websites

Use these tools to view older versions of a website:

* ğŸŒ [Archive.org](https://archive.org/web)
* ğŸ§¾ [Web.Archive](https://web.archive.org)
* ğŸ§Š [Archive.ph](https://archive.ph)
* ğŸ•µï¸ (Optional) [Orkut Archive](https://orkut.google.com) â€” for historical Indian content

---

