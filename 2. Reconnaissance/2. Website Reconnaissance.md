
# 🌐 Website Reconnaissance

**Website reconnaissance** is the process of gathering publicly available information about a website, its infrastructure, and its online presence. It is a common initial step in **penetration testing**, **ethical hacking**, **SEO**, **research**, and **competitive analysis**.

---

## 📘 What is Website Reconnaissance?

Website reconnaissance involves collecting detailed information about a target website or web application. The purpose is to understand its:

* Structure and architecture
* Underlying technologies
* Possible vulnerabilities or exposure points

---

## 🎯 Why Perform Website Reconnaissance?

* **Security Assessment**: Identify vulnerabilities and attack surfaces.
* **Competitive Analysis**: Understand a competitor’s tech stack and strategies.
* **SEO/Marketing Research**: Evaluate site structure, indexing, and backlinks.
* **Compliance Monitoring**: Detect leaked sensitive data and privacy issues.

---

## 🧰 Typical Activities

* **Gather Basic Details**
  IP address, DNS, WHOIS records, SSL certificates.

* **Collect Metadata**
  Title, meta tags, technology stack (e.g., CMS, frameworks, libraries).

* **Identify Exposed Information**
  Emails, phone numbers, hidden directories, usernames.

* **Spidering and Crawling**
  Map internal links and site architecture.

* **Historical Analysis**
  Use archive services like [Wayback Machine](https://archive.org/web/) to view previous versions of the site.

---

## 🔧 Key Tools Used

| Category      | Tools/Examples                         |
| ------------- | -------------------------------------- |
| Online Recon  | Shodan, Censys, BuiltWith              |
| Crawlers      | Screaming Frog, XML Sitemap Generators |
| CLI Tools     | `wget`, `HTTrack` for mirroring        |
| Archive Tools | Archive.org, CachedView                |

---

## 🖥️ Step-by-Step Reconnaissance Tasks

### 1. Visit the Target Website

Manually browse the site to extract:

* Employee details
* Office locations and addresses
* Phone numbers and email addresses
* Usernames (from comment sections, authorship, etc.)

---

### 2. Find Important Files

#### `sitemap.html` — Human-readable index

Helps users navigate the website by listing major sections and links.

#### `sitemap.xml` — Bot Index

Provides URLs for search engines to index. Useful for mapping the site’s structure.

#### `robots.txt` — Restrictions for Search Engines

Specifies directories or pages that should **not** be accessed or indexed by search engines.

---

### 🔗 Example Sitemap URLs

| Website             | Sitemap URL                                                |
| ------------------- | ---------------------------------------------------------- |
| India TV            | `https://www.indiatv.in/cms/sitemap.html`                  |
| MCA India           | `https://www.mca.gov.in/MinistryV2/sitemap.html`           |
| Telangana Transport | `https://www.transport.telangana.gov.in/html/sitemap.html` |

### 🌐 Online Sitemap Generator

Use the following tool to generate or view XML sitemaps:

* [https://www.xml-sitemaps.com](https://www.xml-sitemaps.com)

---

