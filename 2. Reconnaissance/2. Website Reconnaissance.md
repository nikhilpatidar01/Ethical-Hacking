
# 🌐 Website Reconnaissance

**Website reconnaissance** is the process of gathering publicly available information about a website, its infrastructure, and its online presence. It is a common initial step in **penetration testing**, **ethical hacking**, **SEO**, **research**, and **competitive analysis**.

---

## 📘 1. What is Website Reconnaissance?

Website reconnaissance involves collecting detailed information about a target website or web application. The purpose is to understand its:

* Structure and architecture
* Underlying technologies
* Possible vulnerabilities or exposure points

---

## 🎯 2. Why Perform Website Reconnaissance?

* **Security Assessment** – Identify vulnerabilities and attack surfaces.
* **Competitive Analysis** – Understand a competitor’s tech stack and strategies.
* **SEO/Marketing Research** – Evaluate site structure, indexing, and backlinks.
* **Compliance Monitoring** – Detect leaked sensitive data and privacy issues.

---

## 🧰 3. Typical Activities

* **Gather Basic Details** – IP address, DNS records, WHOIS data, SSL certificates
* **Collect Metadata** – Titles, meta tags, CMS/frameworks, JS libraries
* **Identify Exposed Information** – Emails, usernames, hidden paths
* **Spidering and Crawling** – Link mapping and structure analysis
* **Historical Analysis** – View past versions using archive services

---

## 🔧 4. Key Tools Used

| Category      | Tools/Examples                        |
| ------------- | ------------------------------------- |
| Online Recon  | Shodan, Censys, BuiltWith             |
| Crawlers      | Screaming Frog, XML Sitemap Generator |
| CLI Tools     | `wget`, `HTTrack`                     |
| Archive Tools | Archive.org, CachedView               |

---

# 🗂️ 5. Visit the Target Website

Manually browse to extract:

* Names of employees
* Office addresses and phone numbers
* Email addresses and usernames (e.g., in blogs/comments)

---

# 📄 6. Find Important Files

## 6.1 `sitemap.html`

Human-readable site index showing navigation structure.

## 6.2 `sitemap.xml`

Machine-readable sitemap for search engine indexing.

## 6.3 `robots.txt`

Indicates restricted areas for web crawlers.

---

### 6.4 Example Sitemap URLs

| Website             | Sitemap URL                                                |
| ------------------- | ---------------------------------------------------------- |
| India TV            | `https://www.indiatv.in/cms/sitemap.html`                  |
| MCA India           | `https://www.mca.gov.in/MinistryV2/sitemap.html`           |
| Telangana Transport | `https://www.transport.telangana.gov.in/html/sitemap.html` |

---

### 6.5 Online Sitemap Generator

Use online tools to generate/view sitemaps:

* [https://www.xml-sitemaps.com](https://www.xml-sitemaps.com)

---

# 🕸️ 7. Spidering and Crawling — Create Sitemap

## 7.1 What is Spidering?

**Spidering** is the process of using automated bots to systematically follow links across a website.

### 🔍 Purpose

* Discover internal & external links
* Detect hidden directories/pages
* Map the structure of the site

### 🔧 Tools

* **Burp Suite**
* **OWASP ZAP**
* **Arachni**

### 💡 Use Case

Discover pages or endpoints not directly linked from the main site, such as:

* Admin panels
* Hidden forms
* Backup or old files

---

### 7.2 Spidering Process

1. **Starting Point** – Begin with a seed URL (e.g., homepage)
2. **Traversal** – Follow all internal & relevant external links
3. **Data Logging** – Capture all discovered URLs, parameters, endpoints
4. **Scope Control** – Avoid crawling non-target or 3rd-party pages

---

### 7.3 Common Challenges

* **Dynamic Content** – JavaScript-heavy sites may require tools like Puppeteer/Selenium
* **Access Restrictions** – Auth-required pages or IP-restricted content may block spiders

---

## 🌀 8. What is Crawling?

**Crawling** is a broader process that includes spidering and also analyzes the page content.

### 🔍 Purpose

* Download page data
* Extract metadata
* Identify vulnerabilities or hidden assets

> ⚠️ Spidering = Link Mapping
> ⚠️ Crawling = Link Mapping + Content Analysis

---

## 8.1 Features of Crawling

* Build a full index of the website
* Record static & dynamic content
* Analyze page structure, headers, forms

### 🛠 Tools

* **Burp Suite Crawler**
* **OWASP ZAP**
* Python scripts (e.g., using `requests` + `BeautifulSoup`)

---

## 8.2 Crawling Process

1. **Discovery** – Fetch pages and extract links
2. **Depth/Breadth** – Configure how deep and wide to crawl
3. **Data Collection** – Log HTML, assets, API endpoints

---

## 🌐 9. Online Tools for Sitemap & Crawling

* [XML Sitemaps Generator](https://www.xml-sitemaps.com)
* [ConvertCSV URL Extractor](https://www.convertcsv.com/url-extractor.htm)
* [Screaming Frog SEO Spider](https://www.screamingfrog.co.uk/seo-spider/)

---

# 🛠️ 10. Web Crawling Tools

## 10.1 Using **Katana**

### Installation

```bash
git clone https://github.com/projectdiscovery/katana.git
cd katana/cmd/katana
go build
```

### Usage

```bash
katana -u https://www.armourinfosec.com
```

---

## 10.2 Using **Dirhunt**

### Install

```bash
pip install dirhunt
```

### Help

```bash
dirhunt --help
```

### Scan a Website

```bash
dirhunt https://www.armourinfosec.com
```

### Save Output

```bash
dirhunt https://www.armourinfosec.com --to-file /tmp/output.json
```

---

# 💾 11. Locally Mirror the Website — Read Source Code

## 11.1 Using `wget` (Linux)

```bash
wget -m https://www.armourinfosec.com
```

> `-m` = **mirror** — recursive download

---

## 11.2 Using HTTrack

### ✅ On Linux

```bash
apt install httrack
httrack
```

### ✅ On Windows

🔗 [Download HTTrack](https://www.httrack.com/page/2/en/index.html)

Follow GUI steps:

1. Open **WinHTTrack**
2. Create Project
3. Enter URL
4. Select folder
5. Click Start

---

# 🕰️ 12. Wayback Machine — View Old Versions of Websites

Use these tools to view archived versions of a site:

* [Archive.org](https://archive.org/web)
* [Web.Archive](https://web.archive.org)
* [Archive.ph](https://archive.ph)
* [Orkut Archive (Optional)](https://orkut.google.com)

---

