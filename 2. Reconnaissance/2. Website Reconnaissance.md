
# ğŸŒ Website Reconnaissance

**Website reconnaissance** is the process of gathering publicly available information about a website, its infrastructure, and its online presence. It is a common initial step in **penetration testing**, **ethical hacking**, **SEO**, **research**, and **competitive analysis**.

---

## ğŸ“˜ 1. What is Website Reconnaissance?

Website reconnaissance involves collecting detailed information about a target website or web application. The purpose is to understand its:

* Structure and architecture
* Underlying technologies
* Possible vulnerabilities or exposure points

---

## ğŸ¯ 2. Why Perform Website Reconnaissance?

* **Security Assessment** â€“ Identify vulnerabilities and attack surfaces.
* **Competitive Analysis** â€“ Understand a competitorâ€™s tech stack and strategies.
* **SEO/Marketing Research** â€“ Evaluate site structure, indexing, and backlinks.
* **Compliance Monitoring** â€“ Detect leaked sensitive data and privacy issues.

---

## ğŸ§° 3. Typical Activities

* **Gather Basic Details** â€“ IP address, DNS records, WHOIS data, SSL certificates
* **Collect Metadata** â€“ Titles, meta tags, CMS/frameworks, JS libraries
* **Identify Exposed Information** â€“ Emails, usernames, hidden paths
* **Spidering and Crawling** â€“ Link mapping and structure analysis
* **Historical Analysis** â€“ View past versions using archive services

---

## ğŸ”§ 4. Key Tools Used

| Category      | Tools/Examples                        |
| ------------- | ------------------------------------- |
| Online Recon  | Shodan, Censys, BuiltWith             |
| Crawlers      | Screaming Frog, XML Sitemap Generator |
| CLI Tools     | `wget`, `HTTrack`                     |
| Archive Tools | Archive.org, CachedView               |

---

# ğŸ—‚ï¸ 5. Visit the Target Website

Manually browse to extract:

* Names of employees
* Office addresses and phone numbers
* Email addresses and usernames (e.g., in blogs/comments)

---

# ğŸ“„ 6. Find Important Files

## 6.1 `sitemap.html`

Human-readable site index showing navigation structure.

## 6.2 `sitemap.xml`

Machine-readable sitemap for search engine indexing.

## 6.3 `robots.txt`

Indicates restricted areas for web crawlers.

---

### 6.4 Example Sitemap URLs

| Website             | Sitemap URL                                                |
| ------------------- | ---------------------------------------------------------- |
| India TV            | `https://www.indiatv.in/cms/sitemap.html`                  |
| MCA India           | `https://www.mca.gov.in/MinistryV2/sitemap.html`           |
| Telangana Transport | `https://www.transport.telangana.gov.in/html/sitemap.html` |

---

### 6.5 Online Sitemap Generator

Use online tools to generate/view sitemaps:

* [https://www.xml-sitemaps.com](https://www.xml-sitemaps.com)

---

# ğŸ•¸ï¸ 7. Spidering and Crawling â€” Create Sitemap

## 7.1 What is Spidering?

**Spidering** is the process of using automated bots to systematically follow links across a website.

### ğŸ” Purpose

* Discover internal & external links
* Detect hidden directories/pages
* Map the structure of the site

### ğŸ”§ Tools

* **Burp Suite**
* **OWASP ZAP**
* **Arachni**

### ğŸ’¡ Use Case

Discover pages or endpoints not directly linked from the main site, such as:

* Admin panels
* Hidden forms
* Backup or old files

---

### 7.2 Spidering Process

1. **Starting Point** â€“ Begin with a seed URL (e.g., homepage)
2. **Traversal** â€“ Follow all internal & relevant external links
3. **Data Logging** â€“ Capture all discovered URLs, parameters, endpoints
4. **Scope Control** â€“ Avoid crawling non-target or 3rd-party pages

---

### 7.3 Common Challenges

* **Dynamic Content** â€“ JavaScript-heavy sites may require tools like Puppeteer/Selenium
* **Access Restrictions** â€“ Auth-required pages or IP-restricted content may block spiders

---

## ğŸŒ€ 8. What is Crawling?

**Crawling** is a broader process that includes spidering and also analyzes the page content.

### ğŸ” Purpose

* Download page data
* Extract metadata
* Identify vulnerabilities or hidden assets

> âš ï¸ Spidering = Link Mapping
> âš ï¸ Crawling = Link Mapping + Content Analysis

---

## 8.1 Features of Crawling

* Build a full index of the website
* Record static & dynamic content
* Analyze page structure, headers, forms

### ğŸ›  Tools

* **Burp Suite Crawler**
* **OWASP ZAP**
* Python scripts (e.g., using `requests` + `BeautifulSoup`)

---

## 8.2 Crawling Process

1. **Discovery** â€“ Fetch pages and extract links
2. **Depth/Breadth** â€“ Configure how deep and wide to crawl
3. **Data Collection** â€“ Log HTML, assets, API endpoints

---

## ğŸŒ 9. Online Tools for Sitemap & Crawling

* [XML Sitemaps Generator](https://www.xml-sitemaps.com)
* [ConvertCSV URL Extractor](https://www.convertcsv.com/url-extractor.htm)
* [Screaming Frog SEO Spider](https://www.screamingfrog.co.uk/seo-spider/)

---

# ğŸ› ï¸ 10. Web Crawling Tools

## 10.1 Using **Katana**

### Installation

```bash
git clone https://github.com/projectdiscovery/katana.git
cd katana/cmd/katana
go build
```

### Usage

```bash
katana -u https://www.armourinfosec.com
```

---

## 10.2 Using **Dirhunt**

### Install

```bash
pip install dirhunt
```

### Help

```bash
dirhunt --help
```

### Scan a Website

```bash
dirhunt https://www.armourinfosec.com
```

### Save Output

```bash
dirhunt https://www.armourinfosec.com --to-file /tmp/output.json
```

---

# ğŸ’¾ 11. Locally Mirror the Website â€” Read Source Code

## 11.1 Using `wget` (Linux)

```bash
wget -m https://www.armourinfosec.com
```

> `-m` = **mirror** â€” recursive download

---

## 11.2 Using HTTrack

### âœ… On Linux

```bash
apt install httrack
httrack
```

### âœ… On Windows

ğŸ”— [Download HTTrack](https://www.httrack.com/page/2/en/index.html)

Follow GUI steps:

1. Open **WinHTTrack**
2. Create Project
3. Enter URL
4. Select folder
5. Click Start

---

# ğŸ•°ï¸ 12. Wayback Machine â€” View Old Versions of Websites

Use these tools to view archived versions of a site:

* [Archive.org](https://archive.org/web)
* [Web.Archive](https://web.archive.org)
* [Archive.ph](https://archive.ph)
* [Orkut Archive (Optional)](https://orkut.google.com)

---

