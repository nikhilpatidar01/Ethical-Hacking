
# üï∏Ô∏è Waybackurls ‚Äì Archived URL Extractor

**Waybackurls** is a powerful tool by `Tomnomnom` for retrieving archived URLs from the [Wayback Machine](https://archive.org/web/).
It is particularly useful for **security assessments**, as it helps uncover endpoints that may no longer be accessible on the live site but still exist in the archive.

---

## ‚öôÔ∏è Installation

### 1. Install Go (if not already installed)

```bash
apt install golang
```

### 2. Install Waybackurls using Go

```bash
go install github.com/tomnomnom/waybackurls@latest
```

### 3. Move the binary to your system's PATH

```bash
cp -v /root/go/bin/waybackurls /usr/local/bin/
```

---

## üìÑ Basic Usage

### Help Menu

```bash
waybackurls -h
```

### Retrieve URLs for a Single Domain

```bash
waybackurls armourinfosec.com
```

### Using a Full URL

```bash
waybackurls https://www.armourinfosec.com
```

### Use with Multiple Domains

Create a file `domains.txt`:

```bash
vim domains.txt
```

Example contents:

```
armourinfosec.com
thehackersworld.com
```

Run the tool:

```bash
cat domains.txt | waybackurls
```

### Output to a File

```bash
waybackurls https://www.tesla.com > /tmp/tesla-urls.txt
```

---

## üß† Advanced Options

* **List crawled versions** of input URLs:

```bash
cat domains.txt | waybackurls -get-versions
```

* **Show fetch date** in the first column:

```bash
cat domains.txt | waybackurls -dates
```

* **Exclude subdomains**:

```bash
cat domains.txt | waybackurls -no-subs
```

* **Output to a file**:

```bash
cat domains.txt | waybackurls > domains-urls.txt
```

---

## üîç Filtering and Post-Processing

### Extract Unique Domains

```bash
cat /tmp/tesla-urls.txt | cut -d '/' -f3 | sort -u
```

### Exclude Common Static Resources

```bash
grep -vE '\.(css|ico|svg|ttf|eot|woff|png|jpg|jpeg|gif)' domains-urls.txt
```

### Remove URL Parameters (query strings)

```bash
cat domains-urls.txt | cut -d '?' -f 1
```

### Combine Filtering, Deduplication, and Sorting

```bash
grep -vE '\.(css|ico|svg|ttf|eot|woff|png|jpg|jpeg|gif)' domains-urls.txt \
| cut -d '?' -f 1 | sort | uniq
```

### Count the Number of Unique URLs

```bash
grep -vE '\.(css|ico|svg|ttf|eot|woff|png|jpg|jpeg|gif)' domains-urls.txt \
| cut -d '?' -f 1 | sort | uniq | wc -l
```

### Only Include HTTPS URLs

```bash
grep -vE '\.(css|ico|svg|ttf|eot|woff|png|jpg|jpeg|gif)' domains-urls.txt \
| cut -d '?' -f 1 | sort | uniq | grep '^https://'
```

---

## üîó Integrating with HTTP Probing (httpx)

Use [httpx](https://github.com/projectdiscovery/httpx) to check response status:

```bash
grep -vE '\.(css|ico|svg|ttf|eot|woff|png|jpg|jpeg|gif)' domains-urls.txt \
| cut -d '?' -f 1 | sort | uniq | grep '^https://' | httpx
```

### View Results

```bash
cat armourinfosec.com-httpx-output.txt
```

### Filter for Relevant Content

```bash
cat armourinfosec.com-httpx-output.txt | grep -v "Page not found"
```

---

