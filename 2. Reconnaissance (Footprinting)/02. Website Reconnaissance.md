
# ğŸŒ Website Reconnaissance 

Website reconnaissance is the process of gathering public info about a target websiteâ€”used in **ethical hacking**, **penetration testing**, **SEO**, and **competitive analysis**.

---

<img src="https://github.com/nikhilpatidar01/Ethical-Hacking/blob/Master/1.%20Information%20Security/Images/02.%20Website%20Recon.png?raw=true" alt="Website Reconnaissance Diagram" width="100%">


---
## 1ï¸âƒ£ What & Why

**Purpose:**

* Identify structure, tech stack, vulnerabilities
* SEO insights, competitor analysis
* Compliance or data leak check

---

## 2ï¸âƒ£ Information to Collect

* **Basic Info**: IP, DNS, WHOIS, SSL cert
* **Metadata**: Titles, headers, JS libs, CMS
* **Exposed Info**: Emails, usernames, hidden paths
* **Archived Content**: Via tools like Wayback Machine

---

## 3ï¸âƒ£ Important Files to Check

| File           | Purpose                      |
| -------------- | ---------------------------- |
| `sitemap.html` | Human-readable site index    |
| `sitemap.xml`  | Search engine index          |
| `robots.txt`   | Tells crawlers what to avoid |

**Online Sitemap Tools**:

ğŸ”— [xml-sitemaps.com](https://www.xml-sitemaps.com)

**Example Sitemaps**:

* [India TV](https://www.indiatv.in/cms/sitemap.html)
* [MCA](https://www.mca.gov.in/MinistryV2/sitemap.html)

---

## 4ï¸âƒ£ Manual Reconnaissance

Browse the target site to find:

* Employee names, emails
* Office addresses
* Hidden URLs in blogs/comments

---

## 5ï¸âƒ£ Spidering vs Crawling

| Feature   | Spidering                     | Crawling                          |
| --------- | ----------------------------- | --------------------------------- |
| Task      | Link discovery                | Link + content analysis           |
| Tools     | Burp Suite, ZAP, Arachni      | ZAP, Burp Crawler, custom scripts |
| Output    | URLs, directories             | URLs + metadata + content         |
| Challenge | JS-based sites, auth required | Dynamic content, scope control    |

---

### Spidering Steps

1. Start from homepage (seed URL)
2. Follow internal links
3. Log found URLs & parameters
4. Limit to in-scope pages

---

### Crawling Steps

1. Download pages
2. Parse HTML, extract links
3. Collect metadata (headers, forms, scripts)

**Popular Tools:**

* [xml-sitemaps.com](https://www.xml-sitemaps.com)
* [Screaming Frog](https://www.screamingfrog.co.uk/seo-spider/)
* [ConvertCSV URL Extractor](https://www.convertcsv.com/url-extractor.htm)

---

## 6ï¸âƒ£ CLI Recon Tools

### ğŸ”§ Katana (fast crawler)

```bash
git clone https://github.com/projectdiscovery/katana.git
cd katana/cmd/katana && go build
katana -u https://example.com
```

---

### ğŸ”§ Dirhunt (directory scanner)

```bash
pip install dirhunt
dirhunt https://example.com
dirhunt https://example.com --to-file output.json
```

---

## 7ï¸âƒ£ Download Website Locally

### ğŸ”¹ `wget` (Linux)

```bash
wget -m https://example.com
```

> `-m`: mirror mode (recursive)

---

### ğŸ”¹ HTTrack

**Linux:**

```bash
apt install httrack
httrack
```

**Windows:**

[Download](https://www.httrack.com/page/2/en/index.html) > Launch > New Project > Enter URL > Start

---

## 8ï¸âƒ£ View Past Versions (Archives)

Use to explore older site content:

* [archive.org](https://archive.org/web)
* [archive.ph](https://archive.ph)

---

ğŸ“– Reference: Notes inspired by guidance from Mr. Sachin Verma Sir ([Armour Infosec](https://www.armourinfosec.com/)) and enriched with further improvements and updates
